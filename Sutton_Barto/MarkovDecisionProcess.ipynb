{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Markov Decision Processes\n",
    "MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. It involves evaluative feedback similar to armed bandits but also an associative aspect which involves different actions in different situations.\n",
    "<img src=\"markdown_images/agent-environment.png\">\n",
    "<h3 align = center> Agent-Environment Interface</h3>\n",
    "\n",
    "S0;A0;R1; S1;A1;R2; S2;A2;R3; â€¦â€¦. \n",
    "In a finite MDP, the sets of states, actions, and rewards (S,A, and R) all have a finite number of elements. In this case, the random variables Rt and St have well defined discrete probability distributions dependent only on the preceding state and action.\n",
    "\n",
    "**Policies and Value functions**\n",
    "\n",
    "Almost all reinforcement learning algorithms involve estimating value functions -- functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of  â€œhow good\" here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular ways of acting, called policies.\n",
    "\n",
    "**state-value function for policy ğœ‹** :: Expected reward from state s and following policy  ğœ‹ afterwards.\n",
    "<img src=\"markdown_images/state-value.png\">\n",
    "\n",
    "**action-value function for policy  ğœ‹**  :: Expected reward by taking action a from state s and following  ğœ‹ thereafter.\n",
    "<img src=\"markdown_images/action-value.png\">\t\n",
    "\n",
    "**Bellman equation for state value**\n",
    "Basically a walk from current state using the policy and the average reward it receives.\n",
    "\n",
    "<img src=\"markdown_images/bellman-state.png\">\n",
    "\n",
    "**Bellman equation for action value**\n",
    "\n",
    "<img src=\"markdown_images/bellman-action.png\">\n",
    "\n",
    "**Optimal Policies and Optimal Value functions**\n",
    "\n",
    "<img src=\"markdown_images/optimal-policy.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World Problem\n",
    "The cells of the grid correspond to the states of the environment. At each cell, four actions\n",
    "are possible: north, south, east, and west, which deterministically cause the agent to move one cell\n",
    "in the respective direction on the grid. Actions that would take the agent off the grid leave its location\n",
    "unchanged, but also result in a reward of -1. Other actions result in a reward of 0, except those that\n",
    "move the agent out of the special states A and B. From state A, all four actions yield a reward of +10\n",
    "and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'.\n",
    "<img src=\"markdown_images/gridworld.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚  3.31311   â”‚  8.79335  â”‚  4.43144  â”‚  5.32599  â”‚  1.49562  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1.52567   â”‚  2.99631  â”‚  2.25393  â”‚  1.91118  â”‚  0.550869 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  0.0548918 â”‚  0.742147 â”‚  0.676931 â”‚  0.361836 â”‚ -0.399599 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ -0.969506  â”‚ -0.431488 â”‚ -0.351013 â”‚ -0.581873 â”‚ -1.17943  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ -1.85359   â”‚ -1.3412   â”‚ -1.22535  â”‚ -1.41913  â”‚ -1.97146  â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\"\"\"\n",
    "    0 - left(-1,0)\n",
    "    1 - right(1,0)\n",
    "    2 - down(0,1)\n",
    "    3 - up(0,-1)\n",
    "\"\"\"\n",
    "directions = [(-1,0),(1,0),(0,1),(0,-1)]\n",
    "state_A = (1,0)\n",
    "state_Aprime = (1,4)\n",
    "state_B = (3,0)\n",
    "state_Bprime = (3,2)\n",
    "gamma = 0.9 \n",
    "\n",
    "def step(state,action):\n",
    "    direction = directions[action]\n",
    "    if state == state_A:\n",
    "        return state_Aprime,10\n",
    "    if state == state_B:\n",
    "        return state_Bprime,5\n",
    "    next_state = (state[0] + direction[0] , state[1] + direction[1])\n",
    "    if next_state[0] <0 or next_state[0]>=5 or next_state[1] <0 or next_state[1]>=5:\n",
    "        return state,-1\n",
    "    else :\n",
    "        return next_state,0\n",
    "\n",
    "value = np.zeros((5,5),dtype=float)\n",
    "while True:\n",
    "    new_value = np.zeros((5,5),dtype=float)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for action in range(4):\n",
    "                (next_i, next_j), reward = step((i, j), action)\n",
    "                # bellman equation\n",
    "                new_value[i, j] +=  0.25 * (reward + gamma * value[next_i, next_j])\n",
    "    if np.sum(np.abs(value - new_value)) < 1e-2:\n",
    "        break\n",
    "    value = new_value\n",
    "table = tabulate(value.transpose(), tablefmt=\"fancy_grid\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sairam_env)",
   "language": "python",
   "name": "sairam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
