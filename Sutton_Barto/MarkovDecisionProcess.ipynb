{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Markov Decision Processes\n",
    "MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. It involves evaluative feedback similar to armed bandits but also an associative aspect which involves different actions in different situations.\n",
    "<img src=\"markdown_images/agent-environment.png\">\n",
    "<h3 align = center> Agent-Environment Interface</h3>\n",
    "\n",
    "S0;A0;R1; S1;A1;R2; S2;A2;R3; ……. \n",
    "In a finite MDP, the sets of states, actions, and rewards (S,A, and R) all have a finite number of elements. In this case, the random variables Rt and St have well defined discrete probability distributions dependent only on the preceding state and action.\n",
    "\n",
    "**Policies and Value functions**\n",
    "\n",
    "Almost all reinforcement learning algorithms involve estimating value functions -- functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of  “how good\" here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular ways of acting, called policies.\n",
    "\n",
    "**state-value function for policy 𝜋** :: Expected reward from state s and following policy  𝜋 afterwards.\n",
    "<img src=\"markdown_images/state-value.png\">\n",
    "\n",
    "**action-value function for policy  𝜋**  :: Expected reward by taking action a from state s and following  𝜋 thereafter.\n",
    "<img src=\"markdown_images/action-value.png\">\t\n",
    "\n",
    "**Bellman equation for state value**\n",
    "Basically a walk from current state using the policy and the average reward it receives.\n",
    "\n",
    "<img src=\"markdown_images/bellman-state.png\">\n",
    "\n",
    "**Bellman equation for action value**\n",
    "\n",
    "<img src=\"markdown_images/bellman-action.png\">\n",
    "\n",
    "**Optimal Policies and Optimal Value functions**\n",
    "\n",
    "<img src=\"markdown_images/optimal-policy.png\">\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "\n",
    "Calculates the state-value function V(s) for a given policy. In DP this is done using a \"full backup\". At each state, we look ahead one step at each possible action and next state. We can only do this because we have a perfect model of the environment.\n",
    "\n",
    "\n",
    "<img src=\"markdown_images/policy-eval.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World Problem\n",
    "The cells of the grid correspond to the states of the environment. At each cell, four actions\n",
    "are possible: north, south, east, and west, which deterministically cause the agent to move one cell\n",
    "in the respective direction on the grid. Actions that would take the agent off the grid leave its location\n",
    "unchanged, but also result in a reward of -1. Other actions result in a reward of 0, except those that\n",
    "move the agent out of the special states A and B. From state A, all four actions yield a reward of +10\n",
    "and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'.\n",
    "<img src=\"markdown_images/gridworld.png\">\n",
    "\n",
    "\n",
    "##### Policy evaluation for equiprobable policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation for equiprobable policy ::\n",
      "╒════════════╤═══════════╤═══════════╤═══════════╤═══════════╕\n",
      "│  3.36249   │  8.83466  │  4.4715   │  5.36318  │  1.53396  │\n",
      "├────────────┼───────────┼───────────┼───────────┼───────────┤\n",
      "│  1.571     │  3.03591  │  2.29086  │  1.94644  │  0.585912 │\n",
      "├────────────┼───────────┼───────────┼───────────┼───────────┤\n",
      "│  0.0987228 │  0.780693 │  0.712606 │  0.396028 │ -0.365913 │\n",
      "├────────────┼───────────┼───────────┼───────────┼───────────┤\n",
      "│ -0.926015  │ -0.393321 │ -0.315778 │ -0.548119 │ -1.14625  │\n",
      "├────────────┼───────────┼───────────┼───────────┼───────────┤\n",
      "│ -1.81013   │ -1.30313  │ -1.19026  │ -1.38553  │ -1.93847  │\n",
      "╘════════════╧═══════════╧═══════════╧═══════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\"\"\"\n",
    "    0 - left(-1,0)\n",
    "    1 - right(1,0)\n",
    "    2 - down(0,1)\n",
    "    3 - up(0,-1)\n",
    "\"\"\"\n",
    "directions = [(-1,0),(1,0),(0,1),(0,-1)]\n",
    "state_A = (1,0)\n",
    "state_Aprime = (1,4)\n",
    "state_B = (3,0)\n",
    "state_Bprime = (3,2)\n",
    "gamma = 0.9 \n",
    "\n",
    "def step(state,action):\n",
    "    direction = directions[action]\n",
    "    if state == state_A:\n",
    "        return state_Aprime,10\n",
    "    if state == state_B:\n",
    "        return state_Bprime,5\n",
    "    next_state = (state[0] + direction[0] , state[1] + direction[1])\n",
    "    if next_state[0] <0 or next_state[0]>=5 or next_state[1] <0 or next_state[1]>=5:\n",
    "        return state,-1\n",
    "    else :\n",
    "        return next_state,0\n",
    "\n",
    "value = np.zeros((5,5),dtype=float)\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            new_value = 0\n",
    "            for action in range(4):\n",
    "                (next_i, next_j), reward = step((i, j), action)\n",
    "                # bellman equation\n",
    "                new_value +=  0.25 * (reward + gamma * value[next_i, next_j])\n",
    "            delta = max(delta,np.abs(value[i,j]-new_value))\n",
    "            value[i,j] = new_value\n",
    "    if delta < 1e-2:\n",
    "        break\n",
    "table = tabulate(value.transpose(), tablefmt=\"fancy_grid\")\n",
    "print(\"Policy evaluation for equiprobable policy ::\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a nite number of policies, this process must converge to an optimal policy and optimal value function in a nite number of iterations.\n",
    "<img src=\"markdown_images/gridworld_new.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation for Best policy ::\n",
      "╒═════════╤═════════╤═════════╤═════════╤═════════╕\n",
      "│ 21.9775 │ 24.4194 │ 21.9775 │ 19.4194 │ 17.4775 │\n",
      "├─────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ 19.7797 │ 21.9775 │ 19.7797 │ 17.8018 │ 16.0216 │\n",
      "├─────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ 17.8018 │ 19.7797 │ 17.8018 │ 16.0216 │ 14.4194 │\n",
      "├─────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ 16.0216 │ 17.8018 │ 16.0216 │ 14.4194 │ 12.9775 │\n",
      "├─────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ 14.4194 │ 16.0216 │ 14.4194 │ 12.9775 │ 11.6797 │\n",
      "╘═════════╧═════════╧═════════╧═════════╧═════════╛\n",
      "[[1 0 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [1 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\"\"\"\n",
    "    0 - left(-1,0)\n",
    "    1 - right(1,0)\n",
    "    2 - down(0,1)\n",
    "    3 - up(0,-1)\n",
    "\"\"\"\n",
    "directions = [(-1,0),(1,0),(0,1),(0,-1)]\n",
    "gamma = 0.9 \n",
    "def step(state,action):\n",
    "    direction = directions[action]\n",
    "    if state == state_A:\n",
    "        return state_Aprime,10\n",
    "    if state == state_B:\n",
    "        return state_Bprime,5\n",
    "    next_state = (state[0] + direction[0] , state[1] + direction[1])\n",
    "    if next_state[0] <0 or next_state[0]>=5 or next_state[1] <0 or next_state[1]>=5:\n",
    "        return state,-1\n",
    "    else :\n",
    "        return next_state,0\n",
    "\n",
    "def policy_evaluation(policy,value):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                new_value = 0\n",
    "                for action,prob in enumerate(policy[i][j]):\n",
    "                    (next_i, next_j), reward = step((i, j), action)\n",
    "                    # bellman equation\n",
    "                    new_value +=  prob * (reward + gamma * value[next_i, next_j])\n",
    "                delta = max(delta,np.abs(value[i,j]-new_value))\n",
    "                value[i,j] = new_value\n",
    "        if delta < 1e-3:\n",
    "            return value\n",
    "\n",
    "def get_q_values(i,j,value,policy):\n",
    "    Q_values = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        (next_i, next_j), reward = step((i, j), a)\n",
    "        Q_values[a] += reward + gamma * value[next_i, next_j]\n",
    "    return Q_values\n",
    "\n",
    "def policy_iteration():\n",
    "    value = np.zeros((5,5))\n",
    "    policy = np.ones((5,5,4))/4\n",
    "    while True:\n",
    "        policy_stable = True\n",
    "        value = policy_evaluation(policy,value)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                a_old_max = np.random.choice(np.flatnonzero(policy[i][j] == policy[i][j].max()))\n",
    "                Q_values = get_q_values(i,j,value,policy)\n",
    "                if a_old_max != np.argmax(Q_values):\n",
    "                    policy_stable = False\n",
    "                policy[i][j] = np.zeros(4)\n",
    "                policy[i][j][np.argmax(Q_values)] = 1\n",
    "        if policy_stable :\n",
    "            return policy,value\n",
    "policy,value = policy_iteration()\n",
    "table = tabulate(value.transpose(), tablefmt=\"fancy_grid\")\n",
    "print(\"Policy evaluation for Best policy ::\")\n",
    "print(table)\n",
    "print(np.argmax(policy,axis=2).transpose())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sairam_env)",
   "language": "python",
   "name": "sairam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
